
************************************************************************
7/3/25

Fixing things up. Decided that my current approach will be to pick 5 stocks each
morning and trade those.

Picking them based on pre-market movement. Didn't know that was a thing, but stocks
with pre-market movement are more likely to have some volatility during the day.

I told chat gpt I need small and mid-cap stocks, it initially suggested large cap
which makes no sense given that I'm trying to exploit daily price movements, and large
cap stocks are too sturdy.

So anyway, it has me webscraping finviz, a stock site, for the largest movements,
using beautifulsoup, a python library for webscraping. It looks for the stocks
with greater than 5000 sales,

************************************************************************
6/20/25

Ok, the infrsatructure is all set up, the pipeline works, and alpaca makes buy orders. Awesome.

Now I need to specify a few more things:

-I am going to trade during regular open hours, which is 9:30am-4pm EST
-I need the bot to turn on the main loop during this window, and stop when it closes.



************************************************************************
6/6/25

I realized the alpaca-py library is better than the main SDK. I redid
the file that will connect to the API and create the Trading Client. single
point of connection. I will make a 2nd streaming client once I get this first one 
running. 

-added alpaca-py to requirements.txt
-renamed alpaca_client.py and made it a python file to create a trade client
with the secret keys imported from os.environ.
-from alpaca_client import trading_client should be added to any file making 
a trade.

There are different type of market orders you can make. You make them to the same
endpoint, but you pass specifics as an object so you know which thing to buy or sell
etc. There is a section of the api called enums, which can give you the list of
possiblities for each thing. For instance, one enum is called order side I think, 
this is the lingo for which side of the trade do you want, meaning are you buying
or selling.


************************************************************************
6/5/25

Prepping next set of steps. I need to:

-get the alpaca keys accessible to the entry code. I believe this involves
using os.environs given how I injected them into the docker run command.

-determine a simple initial test action to make the connection to the API

-set up a CMD line in the Dockerfile so that the container actually does something
and doesn't exit immediately.

**These have been done. To access the keys passed into the docker run command, you
import os, and run os.environ.get("name of key"). I created main.py, which is the entry
point to the app, and needs the if name = main section.



************************************************************************



*change made for test 4

***
--CI/CD pipeline so that when code is pushed to github, it updates the droplet

	-generate a public/private key locally, add them to github (DONE)	
		*github needs local deploy keys, they were generated with this command:
		ssh-keygen -t ed25519 -C "deploy@tradebot -f ~/deploy_key 
			path: users/andrewlederman/deploy_key (private) and deploy_key.pub (public)
			*the private key should be stored in github secrets(DONE), and the public key
			should be stored in the droplet. So we know where to go, and the private key
			gives github access to the droplet.
	-write a github action that will SSH into the droplet and update the code with rsync	
	
--to create the droplet, I use an SSH key I generated to connect to it. To do this,
you run ssh-keygen on your computer. It generates a private and public key locally. You
paste the pub key into the droplet, and it can then connect to your local machine where
the private key lives.

*the droplet IPv4 is: 137.184.190.88

*I made a 2nd set of ssh keys inside .ssh folder on my computer called trade_bot_rsa that
is the one I actually used.


*******

5/14/25 CI/CD pipeline is set up so that when I push local code to github, it triggers
a github action and sends the code to my digital ocean droplet using rsync. Doing this
required a public/private ssh key connection between github and droplet. The droplet must
have the public key. This will live in root/.ssh/authorized_keys on the droplet. On github,
you put the private key in a secret variable which will then be injected into the action script.

*the private key must include ---BEGIN RSA PRIVATE KEY and ---END RSA PRIVATE KEY lines

To create an SSH key pair, simply run ssh-keygen in a local terminal. It will add a keypair
to your designated folder, ideally have it in .ssh (hidden folder at the user level).
The public key will be called key_name.pub, and the file called key_name is the private key.
You can see this by running cat key_name or cat key_name.pub to see the contents.

The github action runs when code is pushed to main. A github action spins up a virtual
machine called a runner. This virtual machine is what will actually run the commands
you tell it to run in your action script. GitHub actions live in a .github/workflows folder.

**This entire set up, to push local code to github and then to the droplet or another cloud machine,
requires 4 machines: your local machine, the server that hosts github, the virtual machine
runner that spins up when the github action runs, and finally the server with the droplet.
That's a chain of 4 computers required to make this simple set up occur. It's necessary
infrastructure.


********


5/17/25:

I added the alpaca paper account api key and secret key into github. Initially gpt recommended
I manually add a .env file into the droplet, but since I am automating this pipeline,
it said it does make more sense to store the variables in github, and inject them into 
the github action so it will then update the .env file in the droplet from the action.
As of now only the Alpaca paper account is running, so once I'm ready to trade real money
I will need to add a new set of secret keys.


5/27/25:

I updated the GitHub Action to send environment variables to the .env file on the droplet.
The .env file is hidden on the droplet machine, but it is at root/trading-bot. 
ls -a will reveal it.

Some important things to remember: When you create the droplet, to create a secure
connection between the droplet and GitHub, you create an SSH key pair. The public
key is what gets stored on the droplet, and then the secret key lives on GitHub,
as a key to access the droplet. So the public key is more like the address to be accessed,
and the secret key is more like the actual key or password to access that address.

5/28/25:

Talking to GPT, it seems that dockerizing my system on digital ocean would
be a very good idea. It's lightweight, fast, manages dependencies in an easy way.
GitHub Actions would create the Docker image and send it to the droplet, then you
would run the docker command to run it, I think also from the action. It came up
because I realized to use env variables you would need to install python-dotenv,
then for the data analysis also pandas and numpy. So having all of that built into
the image makes it a simpler process.

--Docker is very useful for managing dependency versions. The way to make sure
the dependencies stay working together is to install them all locally and test your system,
then once you have a running app and all the packages are playing well together,
run pip freeze > requirements.txt. What this does is it lists the packages as well
as the version number exactly. This will then get pushed to github, and when the next
docker image is built, it will reference these exact dependencies. So 
requirements.txt is the single source of truth for dependencies and pip freeze will
lock them into a proven configuration.

-The best approach for handling secrets is to have them stored in github secrets,
and in github actions, when I go to run the docker image, I inject all of the 
secrets into the docker run command. By doing this, I can then remove the .env
file which I currently have stored on the droplet. Now the keys are not exposed on
that machine's file system, so github secrets is the only place that has them and
they are never exposed.

-Thinking about security. Will be crucial to be as secure as possible, because
there is no recourse if someone gets in and manages your brokerage account.
Some additional things I will do:
	-Run the container as a non-root user:
		(These lines have been added to Dockerfile):
		RUN useradd -m botuser
		USER botuser

	-use a small/slim base image for Docker (done)
	--Use read-only and no-new-privilegs in the docker file:
		docker run -d --restart always \
	--read-only \
	--name trading-bot \
	--no-new-privileges \
	-e ALPACA_API_KEY=... \
	trading-bot


	-Use SSH key auth only for Droplet, disable password login
	-Set up a firewall for the Droplet as well (Digital Ocean provides one from the dashboard, look into this)
	DigitalOcean Firewall
	Crucial. Do this ASAP.

	Lock down all inbound traffic except:

	Port 22 (SSH)

	Port 443/80 if hosting a dashboard

	Add firewall rules from the DO control panel â†’ Networking â†’ Firewalls

	Blocks most remote exploit attempts cold.

	-rotate Alpaca API keys every 1-3 months

	ðŸ“Š 4. Logging + monitoring
Use docker logs trading-bot to check for anomalies

Set up a basic log watcher:

Pipe logs to file

Monitor for failed trade attempts, bad requests, etc.


I installed docker on the droplet using the commands from the docker docs.
I'm curious about apt, like sudo apt update. I'm starting to understand it a little,
it means advanced package tool. It reaches out to an Ubuntu repository on the internet
that is a repo of commonly used packages. In etc/apt, this folder on linux machines
stores connections to the different packages.

I am adding a firewall. This specifies exactly which ports can access the droplet.
Port 22 is the main one that should be allowed, that is the port for SSH. So if you
try to connect to the specific IP address using SSH protocol, that is always 
connecting on Port 22.

Ok I added the firewall too. Easy to set up, I allowed all traffic to the IP address,
which is common although not entirely secure, but then I allowed only SSH traffic
which is on port 22. So it's like saying, anyone can access this address, but you
can't get in without the secret key.

5/29/25:

Got the full thing to work, Docker CI/CD is running the  container on the droplet.
Some useful commands:
	-docker ps (this will list all running containers)
	-docker logs trading-bot will log everything that has been logged for the container

There were some weird string interpolation issues with variables in the yaml file,
but that got sorted.

The Docker container exits immediately when there is no CMD line. I commented it
out because I didn't want it running anything yet. So fix that and the pipeline should
work.
